<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Jingyi Li</title>
<style>
  body { margin: 0; font-family: Arial, sans-serif; line-height: 1.6; color: #333; }
  header { background: #f8f8f8; padding: 40px 20px; text-align: center; }
  header img { width: 140px; border-radius: 50%; }
  nav { background: #333; color: #fff; padding: 10px 0; text-align: center; }
  nav a { color: #fff; margin: 0 15px; text-decoration: none; font-size: 16px; }
  section { padding: 60px 20px; max-width: 900px; margin: auto; }
  h2 { border-bottom: 2px solid #ddd; padding-bottom: 8px; }
  .social a { margin: 0 10px; }
  .pub-list li { margin: 8px 0; }
</style>
</head>
<body>

<header id="home">
  <img src="jingyili.jpg" alt="Your Name">
  <h1>Jingyi Li</h1>
  <p>Ph.D. Candidate | Researcher in Machine Unlearning & Federated Learning & LLM</p>
  <div class="social">
    <a href="https://scholar.google.com/citations?user=2UJK7RQAAAAJ">Google Scholar</a>
    <a href="https://github.com/jingyi-li-bjtu">GitHub</a>
    <a href="mailto:22110139@bjtu.edu.cn">Email</a>
  </div>
</header>

<nav>
  <a href="#about">About</a>
  <a href="#publications">Publications</a>
  <a href="#research">Research</a>
  <!-- <a href="#contact">Contact</a> -->
</nav>

<section id="about">
  <h2>About Me</h2>
  <p>
    I am a Ph.D. candidate at Beijing Jiaotong University, supervised by Prof. Yidong Li. My research focuses on Machine 
    Unlearning, Federated Learning, Large Language Models (LLMs), and Trustworthy AI. I am currently a visiting student 
    at the Applied Machine Learning (AML) Lab, City University of Hong Kong, where my recent work concentrates on unlearning techniques and related 
    security and privacy-preserving methods in LLMs and Multimodal Large Language Models (MLLMs).
  </p>
</section>

<section id="publications">
  <h2>Selected Publications</h2>
  <ul class="pub-list">
    <li>
      Li, J., Li, Y., Wu, J., Zhang, Z. Ji, Y. (2024). Blockchain-Based Decentralized Cloud Storage with Reliable Deduplication and Storage Balancing.
      <em>IEEE Transactions on Network Science and Engineering (TNSE)</em>.
    </li>
    <li>
      Li, J., Zhang, Z., Zhang, H., &amp; Li, Y. (2025). Federated Metric Learning and Machine Unlearning Based on Prototype Distillation.
      <em>IEEE Transactions on Services Computing (TSC)</em>.
    </li>
    <li>
      Li, J., Li, Y., Ding, C., Yu, J., & Ren, Y. (2022).
      Identity-based secure and efficient intelligent inference framework for IoT-cloud system.
      <em>IEEE 13th International Symposium on Parallel Architectures, Algorithms and Programming (PAAP)</em>.
    </li>
    
    <li>
      Li, J., Zhang, Z., Li, Y., Guo, X., & Li, H. (2021).
      FIDS: Detecting DDoS through federated learning based method.
      <em>IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)</em>.
    </li>
    
    <li>
      Li, J., Zhang, H., & Li, Y. (2021).
      Parallel detection architecture for long-tail problem in intelligent transportation systems.
      <em>IEEE International Conference on Digital Twins and Parallel Intelligence (DTPI)</em>.
    </li>
    
    <li>
      Li, J., Wu, J., & Chen, L. (2018).
      Deduplication with blockchain for secure cloud storage.
      <em>CCF Conference on Big Data</em>.
    </li>
    
    <li>
      Li, J., Wu, J., Chen, L., & Li, J. (2018).
      Blockchain-based secure and reliable distributed deduplication scheme.
      <em>International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP)</em>.
    </li>

    <!-- Add more -->
  </ul>
</section>

<!-- <section id="research">
  <h2>Research</h2>
  <p><strong>Federated Machine Unlearning via Prototype Distillation</strong></p>
  <p>
    A framework to enable effective unlearning in federated settings by transforming local data into prototypes
    and aligning them with global representations, supporting different unlearning granularities.
  </p>

  <p><strong>Transmission-Efficient Embedding Unlearning</strong></p>
  <p>
    Techniques that reduce communication and computation costs for unlearning in large pretrained models
    with limited data and resource constraints.
  </p>
</section> -->

<section id="contact">
  <h2>Contact</h2>
  <p>Email: your_email@domain.com</p>
  <p>
    Feel free to reach out for collaboration, research discussions, or inquiries.
  </p>
</section>

</body>
</html>
